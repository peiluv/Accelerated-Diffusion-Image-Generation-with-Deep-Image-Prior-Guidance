{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DIP（Deep Image Prior）**\n",
    "\n",
    "- 由 Dmitry Ulyanov 等人在 2018 年提出的一種影像處理方法。它提出一個革命性的觀點：**不需要大量訓練資料，只用一個隨機初始化的 CNN，本身就能作為圖像的「先驗」知識**，用來解決圖像重建問題（如去雜訊、超解析、影像修復等）。\n",
    "\n",
    "- **DIP = 不用訓練資料的影像重建方法，只靠一個神經網路的結構本身來恢復圖像。**\n",
    "\n",
    "- 傳統方法會學習大量圖片分布，建立明確的「先驗」（如 GAN、DDPM），而 **DIP 的大膽想法是：CNN 的結構本身就是一種 implicit prior**，代表**一個隨機初始化的 CNN，天生就傾向於生成自然圖片的統計特性（如連續、平滑、局部結構），而不是雜訊。**\n",
    "\n",
    "| 名稱 | Deep Image Prior (DIP) |\n",
    "|------|------------------------|\n",
    "| 發表年份 | 2018 |\n",
    "| 作者 | Ulyanov, Vedaldi, Lempitsky |\n",
    "| 核心想法 | CNN 結構本身提供影像先驗，不需大數據 |\n",
    "| 特點 | 單圖像訓練，無需資料集 |\n",
    "| 關鍵技術 | Random CNN + Loss + Early stopping |\n",
    "\n",
    "\n",
    "### **核心原理**\n",
    "\n",
    "- CNN 對雜訊的表現力 **遠小於** 對自然圖像的表現力。\n",
    "- 隨機初始化的 CNN 一開始擅長產生平滑與結構化的模式，而不是雜訊。\n",
    "- 所以，只要不訓練太久，它會先學會圖片結構，**還沒來得及學會雜訊就停下來**，就能恢復清晰的圖像。\n",
    "- **Early Stopping 是關鍵**：需要觀察生成圖的變化，挑選最佳時機\n",
    "    - 太早停：圖像還沒還原好\n",
    "    - 太晚停：CNN 開始「記住」雜訊，導致 overfitting\n",
    "\n",
    "\n",
    "### **目標：還原一張 degraded image（受損圖像）**\n",
    "\n",
    "1. 建立一個隨機初始化的 CNN（例如 U-Net）。\n",
    "2. 給這個網路一個隨機向量（如噪音）作為輸入。\n",
    "3. 設定一個 loss function，度量輸出和原始損壞圖片的差異。\n",
    "4. 不斷更新 CNN 的權重，使得輸出接近受損圖像，但不 overfitting 雜訊。\n",
    "\n",
    "\n",
    "### **應用**：\n",
    "\n",
    "1. **圖像去雜訊（Denoising）**\n",
    "    - 給一張含雜訊的圖片作為目標\n",
    "    - 使用 CNN 輸出逐漸貼近原圖\n",
    "    - 關鍵：在模型開始 overfitting 雜訊前停止訓練（early stopping）\n",
    "\n",
    "2. **圖像修補（Inpainting）**\n",
    "    - 把部分遮住的圖片當作目標\n",
    "    - Loss fn 只對「未遮住」的部分做誤差計算\n",
    "    - CNN 會自動生成符合結構的補足圖像\n",
    "\n",
    "3. **圖像超解析（Super-resolution）**\n",
    "    - 低解析度圖片作為目標\n",
    "    - CNN 預測高解析度結果並與低解析縮小版本比較\n",
    "\n",
    "### **視覺化過程**\n",
    "\n",
    "```plaintext\n",
    "[隨機向量 z]\n",
    "      ↓\n",
    "CNN (隨機初始化)\n",
    "      ↓\n",
    "  產生圖片 f_θ(z)\n",
    "      ↓\n",
    "與受損圖像比較 loss\n",
    "      ↓\n",
    "更新 θ (gradient descent)\n",
    "      ↓\n",
    "重複多次 → 重建原圖\n",
    "```\n",
    "\n",
    "\n",
    "### **比較其他方法**：\n",
    "\n",
    "| 特性 | DIP | GAN | DDPM |\n",
    "|------|-----|-----|------|\n",
    "| 是否需大量資料 | ✘ 不需要 | ✔ 需要 | ✔ 需要 |\n",
    "| 是否訓練整個模型 | ✔ 每張圖都訓練一次 | ✔ 訓練一次可用於多圖 | ✔ 同上 |\n",
    "| 圖像品質 | 高（需控制早停） | 高（但易不穩） | 極高（但慢） |\n",
    "| 適用於 | 圖像復原 | 圖像生成 | 圖像生成 |\n",
    "\n",
    "\n",
    "| 概念 | 說明 |\n",
    "|------|------|\n",
    "| U-Net | 一種 U 型 CNN，包含對稱的 Encoder 與 Decoder |\n",
    "| Encoder | 負責特徵萃取（Down-sampeling） |\n",
    "| Decoder | 負責圖像恢復（Up-Sampeling） |\n",
    "| Skip Connection | 將 Encoder 特徵傳遞到 Decoder，保留圖像細節 |\n",
    "| 應用 | 醫學影像分割、DIP、DDPM、Stable Diffusion 等 |\n",
    "\n",
    "\n",
    "## **U-Net**\n",
    "\n",
    "- 一種特殊設計的 CNN 架構，最初由 Olaf Ronneberger 等人在 2015 年為醫學影像分割任務所提出，但後來廣泛應用於各種影像處理任務（如：DIP、DDPM、圖像修補、超解析等）。\n",
    "\n",
    "- **特色**\n",
    "1. 它的名字來自於其**對稱的 U 型結構**\n",
    "2. **U-Net = Down-sample + Up-sample + Skip Connections**\n",
    "3. 在 U-Net 中，每次 Encoder 的輸出，都會跳過中間 Bottleneck，直接接到對應 Decoder 層。\n",
    "\n",
    "```plaintext\n",
    "輸入圖像\n",
    "   ↓\n",
    "[Encoder]（逐步 Down-sampling）\n",
    "   ↓\n",
    "    ──────⟶ Bottleneck ⟶\n",
    "   ↑                          \n",
    "[Decoder]（逐步 Up-sampling）\n",
    "   ↑\n",
    "輸出圖像\n",
    "```\n",
    "\n",
    "- **Encoder - 萃取高層次特徵**，透過：\n",
    "    - 卷積層（Convolution）\n",
    "    - ReLU acti_fn\n",
    "    - 最大池化（Max Pooling）\n",
    "    - 逐步**降低空間解析度**，但增加特徵 channel 數量\n",
    "\n",
    "- **Decoder - 將特徵圖恢復回原始尺寸**，透過：\n",
    "    - 反卷積（Transposed Convolution）或上採樣（Up-sampling）\n",
    "    - 卷積層（Convolution）\n",
    "    - 其任務是**重建原始影像的空間資訊**\n",
    "\n",
    "## **Skip Connections -> U-Net 最關鍵的創新方法**\n",
    "\n",
    "- 在 Encoder 的每一層輸出，**會直接連接到對應的 Decoder 層**，通常是**將 feature maps 做 concat（拼接）** 或加法運算，這樣做的目的是讓 Decoder：\n",
    "    1. 不只是**從「抽象特徵」重建圖片**\n",
    "    2. 還能**結合「原始的細節資訊」，讓輸出更清晰、更準確**\n",
    "\n",
    "- **為什麼需要它？**：在壓縮的過程中，我們會丟失很多「低層次資訊」（如邊緣、細節等），如果沒有 skip connection，就很難完全恢復這些細節 -> **跳過中間 Bottleneck**\n",
    "\n",
    "- **Skip connection 解決了這個問題：**\n",
    "    - 保留了原圖的空間資訊\n",
    "    - 加速訓練與收斂\n",
    "    - 防止深層網路中的梯度消失（類似於 ResNet 的思想）\n",
    "\n",
    "```plaintext\n",
    "Input\n",
    "  │\n",
    "[Encoder1] ─────────────┐\n",
    "  ↓                     │\n",
    "[Encoder2] ───────┐     │\n",
    "  ↓               ↓     ↓\n",
    "[Bottleneck]      ↑     ↑\n",
    "  ↓               │     │\n",
    "[Decoder2] ◄──────┘     │\n",
    "  ↓                     │\n",
    "[Decoder1] ◄────────────┘\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "| 模型 | 特點 | 是否使用 Skip Connection |\n",
    "|------|------|----------------------------|\n",
    "| CNN | 特徵萃取 | ✘ 無 |\n",
    "| ResNet | 深度學習中防止梯度消失 |  ✔（加法） |\n",
    "| U-Net | 影像重建與分割 | ✔（拼接） |\n",
    "| DIP | 使用 U-Net 結構做去雜訊 | ✔ |\n",
    "| DDPM | 通常用 U-Net 當作噪音預測器 | ✔ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv => BN => ReLU) * 2\n",
    "    U-Net 的基本 building block，對輸入的 feature 進行初步的特徵提取和轉換，改變 C 並引入非線性\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "    def forward(self, x):\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv ->  U-Net 的 encoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "    def forward(self, x):\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv ->  U-Net 的 decoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        '''\n",
    "        scale_factor=2：\n",
    "            - 如果輸入的尺寸是 (batch_size, channels, height, width)\n",
    "            - 那麼輸出的尺寸將是 (batch_size, channels, 2*height, 2*width)\n",
    "\n",
    "        align_corners：決定在調整尺寸時是否對齊角點像素\n",
    "            - True: 輸入和輸出張量的角點像素將被對齊，並在調整尺寸時被保留，在某些情況下可以提供更精確的幾何對應\n",
    "            - False (預設值): 角點像素可能不會完全對齊。\n",
    "        在 mode='bilinear' 或 'bicubic' 模式下，建議將 align_corners 設置為 True，以獲得更一致的結果，尤其是在進行多個連續的上採樣或下採樣操作時\n",
    "        '''\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        '''\n",
    "        torch.cat -> Skip-connection：\n",
    "        將 Up-Sampeling 後的特徵圖 (x1) 與來自 Encoder 路徑中對應層的、具有更高分辨率的特徵圖 (x2) 在通道維度上連接起來\n",
    "        這樣 Decoder 就能從 Encoder 中提取細節信息，有助於生成更精細的輸出\n",
    "        '''\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"U-Net 的輸出層 -> 將 U-Net 最終的特徵表示映射到所需的輸出圖像空間\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "    def forward(self, x):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Down-sampling**\n",
    "\n",
    "- 將原始影像或特徵圖的空間解析度 **縮小**（例如從 256×256 降為 128×128），以便：\n",
    "    - 降低計算成本\n",
    "    - 萃取**更抽象**的特徵\n",
    "\n",
    "- **目標**：學會圖像的高階語義（what is it?），而非低階細節（where is it?）\n",
    "\n",
    "### **常見的 Down-sampling 方法：**\n",
    "\n",
    "| 方法 | 說明 |\n",
    "|------|------|\n",
    "| **Max Pooling** | 每個區塊取最大值（常用於 CNN） |\n",
    "| **Average Pooling** | 每個區塊取平均值 |\n",
    "| **Strided Convolution** | 使用步長 > 1 的卷積自動達到下採樣效果 |\n",
    "| **Resize + Convolution** | 先縮圖再卷積（較少見） |\n",
    "\n",
    "- 以 Max Pooling 為例，對 4×4 特徵圖使用 2×2 視窗進行下採樣：\n",
    "\n",
    "```plaintext\n",
    "原圖：          ↓ MaxPool 2x2\n",
    "[1 2 5 6]       [5 8]\n",
    "[4 5 8 7]  →    [9 7]\n",
    "[9 2 3 1]\n",
    "[3 9 7 0]\n",
    "```\n",
    "\n",
    "## **Up-sampling**\n",
    "\n",
    "- 將特徵圖的空間解析度 **放大**（例如從 64×64 變成 128×128），**使最終輸出符合原始圖像大小**。\n",
    "\n",
    "- **目標**：恢復圖像的細節與空間分布，用於圖像生成或復原\n",
    "\n",
    "### **常見的 Up-sampling 方法：**\n",
    "\n",
    "| 方法 | 說明 |\n",
    "|------|------|\n",
    "| **Nearest Neighbor** | 每個像素複製填滿更大的區塊（快速但粗糙） |\n",
    "| **Bilinear / Bicubic Interpolation** | 根據鄰近像素做插值（平滑） |\n",
    "| **Transposed Convolution**（Deconv） | 卷積的反操作，會學習上採樣的權重（常見於 U-Net） |\n",
    "| **Pixel Shuffle**（亞像素卷積） | 將通道資訊轉回空間尺寸（用於超解析） |\n",
    "\n",
    "- 以 Nearest Neighbor 為例，對 2×2 特徵圖放大至 4×4：\n",
    "\n",
    "```plaintext\n",
    "原圖：         上採樣結果：\n",
    "[1 2]          [1 1 2 2]\n",
    "[3 4]     →    [1 1 2 2]\n",
    "               [3 3 4 4]\n",
    "               [3 3 4 4]\n",
    "```\n",
    "\n",
    "| 名稱 | Down-sampling | Up-sampling |\n",
    "|------|----------------|-------------|\n",
    "| 目標 | 壓縮資訊、抽象理解       | 還原圖像、精細輸出       |\n",
    "| 功能 | 降低尺寸、萃取語義 | 增加尺寸、恢復細節 |\n",
    "| 方法 | MaxPool、Strided Conv | Nearest、Bilinear、Transposed Conv |\n",
    "| 應用 | 特徵萃取、分類前處理 | 圖像恢復、影像生成 |\n",
    "| 缺點 | 可能丟失細節 | 模糊、格狀 artifacts |\n",
    "| 使用場景 | CNN Encoder、ResNet 等   | U-Net Decoder、GAN、VAE |\n",
    "\n",
    "\n",
    "### **與 Skip Connection 的搭配**\n",
    "\n",
    "- U-Net 就是透過下採樣學到抽象語義，再透過上採樣還原圖片，並搭配 Skip Connection 將低層細節直接傳遞回來，達到又準又細緻的輸出。\n",
    "\n",
    "```plaintext\n",
    "[Input]\n",
    "   ↓\n",
    "Down-sampling (MaxPool / Strided Conv)\n",
    "   ↓\n",
    "[抽象表示] ← Skip Connection ← Encoder features\n",
    "   ↓\n",
    "Up-sampling (Transposed Conv)\n",
    "   ↓\n",
    "[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet architecture for DIP\"\"\"\n",
    "    def __init__(self, input_channels, output_channels, hidden_size, bilinear=True):\n",
    "        '''\n",
    "        bilinear=True：決定在 Up-Sampeling 時使用哪種插值方法\n",
    "            - True: 使用雙線性插值 (bilinear interpolation) 一種平滑的 Up-Sampeling\n",
    "            - False: 使用轉置卷積 (transposed convolution) 轉置卷積可讓網路學習如何進行 Up-Sampeling\n",
    "        '''\n",
    "    def forward(self, x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepImagePrior(nn.Module):\n",
    "    \"\"\"Deep Image Prior model wrapper\"\"\"\n",
    "    def __init__(self, args):\n",
    "    def forward(self, noise):\n",
    "        \"\"\"通過神經網路將 noise 轉換為 image\"\"\"\n",
    "\n",
    "    def get_random_input(self, target_shape, device='cuda'):\n",
    "        \"\"\"\n",
    "        Generate random input noise for the DIP model\n",
    "        根據 target_shape 反推 input noise size\n",
    "        假設每層 UP-Sampeling x2，則 input noise size 為 H/(2^layers), W/(2^layers)\n",
    "        \"\"\"\n",
    "\n",
    "    def generate_prior(self, target_image, iterations, reg_noise_std, device='cuda'):\n",
    "        \"\"\"Generate Prior for DIP-Guided DDPM\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
